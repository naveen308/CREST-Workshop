{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
   
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq_bCwUE2KaU"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZRsZho82ivv",
        "outputId": "b92abb6a-b77a-4645-e1f4-e43fd7cb2220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRqjTk0m3pCK",
        "outputId": "d7c5a482-f68a-454d-a4a8-b19d993517da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "gk1J5CxF3uDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "LE3p80l94HX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"truthful_qa\", \"generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "bbec35721c0841bd833b68743623624c",
            "213b32092a8c484794cda35a054ae8fc",
            "67bb4577344b48f6bf265c70de145c14",
            "5f505566ffd6438492435c938a2cee3a",
            "86f37f10920a4c3683bfd4c2473ce2a4",
            "bef2571a49fb40a8bc046fedcd6a989e",
            "b7d23d103165444da1994ce804f14fc7",
            "168c4086af29449f92b23d7640f2b8fd",
            "61f50422381f45ae839582220d8d5fae",
            "70331beaefcd45759fdf5b65ccd9964f",
            "bc345258546b45428396d7bae479d2ed",
            "a4a55f91d8cc4a2888fa3179e9d81397",
            "23460f2903484e2e8833840bd305c577",
            "ec83c2755507415db409f8943d86649c",
            "3fb1dcda1a0c4b348fc9044d78675698",
            "7ac488808c1c44dfbdcbbead710869d6",
            "6d654c8aaa894a4b8b37d56feacd2cc1",
            "2ad705495fde4bd7ba31c70ea49e89b6",
            "cbe12e33d6d840b1993ce1e55ceda7bc",
            "946c54a819704624b876dce26a1c37c7",
            "a7eb2d22f79940ab9dd74f52604ec31b",
            "3a987fab08c14fd4aea8eb43f6d95c11",
            "f1decc8bcf9142daa1cd3d50738fb328",
            "2a7bae55ff6e4a7699c0836166470945",
            "91fefed7f84342c188a6f326f2867783",
            "3b5f3d964c334fd6ba0468cc91a3af51",
            "3ab3c6ad62ee4d4cbb8d3e4a1e6585e2",
            "72392c291481413fae67d321cebd7289",
            "eb775153dfda4b4eac4d078ab5e5228b",
            "aaa62310572b462aad7d108940a60956",
            "b0f4f4c9012d4d1abf92505a3e98bf97",
            "5a80d736453d42789b79bc57559e48bf",
            "cf44e9495de14b84b210ecb52b1da1d1"
          ]
        },
        "id": "yGtTj5GT4RSM",
        "outputId": "7a75a059-3ea0-4fdb-c1fd-c1fdb58074e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbec35721c0841bd833b68743623624c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/223k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4a55f91d8cc4a2888fa3179e9d81397"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/817 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1decc8bcf9142daa1cd3d50738fb328"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "a9be074999a442d09a167be8d26058ad",
            "e74386d227fd4273b39993c780836896",
            "2476c62de54e40df8a82c2dda8970825",
            "d5c4c1d208884d629c0d1a5424e040a9",
            "4d9061e5fdbb45709a31e5b6bb1c5dd0",
            "db61f7ea32684186bacdc3e394e30f3e",
            "3c3d7a9380194b05955656048159fa3e",
            "f7c727b2bc2c4bcaa1c5c09748ab77b5",
            "ead6c434b4f6449ea55d75dbed737c8c",
            "576ab333c4c84a2eac39c0278aa2e1c1",
            "06a0a96ff43a4011a8c0c3042bf440bb",
            "c702564af9e3457698013110d615fc3f",
            "4f08b66d8f484e53945d3f17fac6b7c3",
            "ab503633b57245d99820049c5d779d34",
            "3fd05c5601594b508f27d05e22ea0c90",
            "734f1e48ee6b444088e837feeead78d0",
            "cc1c20110015439cb5c688d2af18c6e9",
            "11a0f9bd0f4f41629698d3d17c71e741",
            "da00cec19435404db06df68cc89629ba",
            "cb0cb1faab8743b9a66d85e6497c1952",
            "7db0a463151a4f5dab54cd00ca0c7e9b",
            "84c575deeda14a82a4f909e809542916",
            "5e02dbaa358e455e8c13e62cdbcccecc",
            "c1132e596db14169a24db0b8599c9e16",
            "516a9c535a244f80b1c3eed90d5ebb33",
            "88692f5dc578442aad6804a4cc211acc",
            "bd29264285a34e77bf63fe22210d92f8",
            "87b32b40c00f489fb08691948f7d8761",
            "31690e748d374e91bdd4a1d4399d625c",
            "543113cb46bb4e5791b028d898404baa",
            "faa7b2e897b0461987dc381cb70f8dfa",
            "0afc1988dbe84a4e8659bbc2a17b5139",
            "62115872ed344fb896a932d704a31866",
            "f32bfbe68824456082e2c7c6e8a1c8e3",
            "d15a8188091c4335826b6758f895671a",
            "103c6a7e858340f19e749c83427fa2e5",
            "1fc849fa0947472f8a472443b0fcd122",
            "dc9934605f404b84aa45927fe5d80fb7",
            "7319e26ad7814159938ccb46550997f5",
            "3ac75d452e264dce8e2e7d06de23adec",
            "b973a614ff0b4ba181d35562540fdd6d",
            "c18d2daf336e4b229047417e9b68da61",
            "38d2296c05924dcbb18c621af3bae328",
            "bd269dd2bbaa4c07b6f172e732422d3e",
            "8261bdcaa4e24e59b7acd01c629c2dd7",
            "1bf81e12c3c046f089a09cdc67949c52",
            "fe50d98a4d154beca80e6970f6e6d5ff",
            "879a4a9275934f4288ecae2ca6ae8ed0",
            "20199885a88740eeb0976235d861e0e1",
            "56213ff5a1f64ce28b69dc15de7c96de",
            "1b87f055c39b4761bb372ac2d953bdfb",
            "884eebbfbd88459aa552a92d63fb76df",
            "c8b695b824324c789d0c7a5e882e577a",
            "9f603b29f82d488f8eb0d8f450b56782",
            "f13659f5e23146b990c84ac1d233e566",
            "9715f0623b6e4576b1bb5642ae6b9eae",
            "0f2069c0d7eb4a98b3b9f51f5fe699b8",
            "4bb7e21ca8e04243a8a729a9c5164afe",
            "ade4c7b3d6e14471876343a5fb272da0",
            "dcff4ab62f93479485209d821d1964bd",
            "a798a58969004b75a88f873aaea9e189",
            "09a911944e42421bb8aa9aa546e458b4",
            "6f66014bd777454ba6e771c48e7a655b",
            "19cc73e52dbb4286ace34e8988e9fe83",
            "62e27fd777b4475ab0e7c3cf9d4d2200",
            "49503b43d23e44d8a17a33dc82c1427f",
            "8ef32303ff734d6db39743cf905db37f",
            "7da009e098d343f59cd36f5976c8f4fc",
            "d708592ce020447f915683fc5fb8bc7e",
            "24c178d2347f4c6591491abcb88f0118",
            "58d5895100914103a85baaa8ab368aa8",
            "a2a7c60587564e95a2e71cdeaebd0ffe",
            "d38382a87304456383e3ceebd023fffa",
            "2f673ae997b942989db0ef8120a53a10",
            "dd2017653f6f44768bdd4dddff899a3d",
            "b07f66b7abeb4bc4be56fdf397d1999f",
            "16940c28ea8d427489d69f27a450d5f7",
            "8b7b60a824184b77a6fc70c62aecc7f3",
            "b9f698904ebd4edf9479891675b94ea9",
            "b62dc77ff2354b678424cfe1c7c73cbc",
            "7b6360a2c34040d09a7b412744ca1c32",
            "a2918f1403d24dbd899aa639deee71a6",
            "d7f52de2d649442d8c8a409d7516c559",
            "aca9970734c74ad88e3e2b602b86cd7e",
            "86cfc9453aac4ceebe9827aaae26ac04",
            "0d2c5711254a4814b5fa834f27b6fbf8",
            "2fee6fb7573943d3a3f9e3e101e191aa",
            "20ff9e13b2d64270a3665c62cde227ce",
            "7c5195744695483e847f137576d49417",
            "ca30a818ccc949748256e4591a407ce4",
            "07fc5224a1a044a9b72b1010ec5ec822",
            "b012608e016c4028b57aa97c734c7066",
            "90128792c84346099f7584a40b60a871",
            "ec3a4f2ead774d3e8e58eae7c7cdfd0c",
            "ff07ecbb269b49a7971689ceae37ca87",
            "b85359364ab84c56ba4f31aefef27408",
            "bbce7d448a1545dc8d68334ea18b7ca5",
            "8b25334654a94642af4798833c6a3104",
            "cf8e58ace2de4079936b7693a6d35c03",
            "a28797cb33b44063b9760cf3104c1369",
            "7b16b4e7a6934057a42fc3209f13e55b",
            "94151193b86a4e6c86cc4428c0a5acf8",
            "cb9432fbaecd4e8cb42757e939287b5f",
            "ecb47e61ad424675953c83a319896b3a",
            "6531031f09b84c70895ed4dda9da47a2",
            "3f52b4d5dab6478bb8642c3dce13232b",
            "78afe7fe217843b781d9dfd1f08909c5",
            "5327ad7bbde74f0d862c322e8ba1b64d",
            "50ad64892829476d9eaf896f23684b66",
            "7fcc287b01d54dc7a8874d10f4091ec3",
            "218deb0cd2cf49d4aa6c96f08caf9664",
            "f4ae9ce23e9648298ce3ebde2c45da7e",
            "415842e4e36d4efe8a8460edf941575a",
            "74e7b899500a4c4f9af365afc9dbf63a",
            "1a5efe28ea554c47a423e1b1028c715d",
            "ee7182d0b6c44cceb597e0c96c7747cd",
            "b1ecd979efbb417896408c09506b520b",
            "6fdc4d2e441d46139ba752263efaf91f",
            "22a86092b9ff4cc4971680bcafca069b",
            "41c77638fe6b45ea90de55e8e5869849",
            "a68367981f5943d2a54f1ffc41f87253"
          ]
        },
        "id": "S2c5pZPy4VUU",
        "outputId": "4729ab5f-76f2-48ef-9eac-5b8242b03b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9be074999a442d09a167be8d26058ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c702564af9e3457698013110d615fc3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e02dbaa358e455e8c13e62cdbcccecc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f32bfbe68824456082e2c7c6e8a1c8e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8261bdcaa4e24e59b7acd01c629c2dd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9715f0623b6e4576b1bb5642ae6b9eae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ef32303ff734d6db39743cf905db37f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b7b60a824184b77a6fc70c62aecc7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c5195744695483e847f137576d49417"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a28797cb33b44063b9760cf3104c1369"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "218deb0cd2cf49d4aa6c96f08caf9664"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_answers = dataset['validation']['best_answer']\n",
        "train_embeddings = model.encode(train_answers, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "393fgoGm4iEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_answer(query, embeddings, answers, top_k=1):\n",
        "  \"\"\"\n",
        "  Retrieves the most similar answer from the knowledge base to the query.\n",
        "  \"\"\"\n",
        "  query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "  # Calculate cosine similarity between the query and all answer embeddings\n",
        "  cosine_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
        "  # Get the top k most similar answers\n",
        "  top_results = torch.topk(cosine_scores, k=top_k)\n",
        "\n",
        "  retrieved_answers = [answers[idx] for idx in top_results.indices]\n",
        "  return retrieved_answers"
      ],
      "metadata": {
        "id": "VYCVar4p40CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util"
      ],
      "metadata": {
        "id": "K73q79-l5B8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259,
          "referenced_widgets": [
            "4981ece55e4d4781a3a5a76300ae1c0b",
            "bdc7e3c43f4b4875a531d91ded48a98f",
            "b62ae64dc8314606b1105f63167fb66a",
            "daebf40fb0744b988ba08f3b1a317c80",
            "78f8993f0a7b4ea4a805f843e303b647",
            "fee16591ad304599857f77e44abd5c18",
            "035e5b58610e4132b2622678513c94ee",
            "7ffbb23fbda74cb7af08f3e6da53bc27",
            "90483e9eab094eb78a4577e36fa9d55a",
            "fa8c389cf74b4416b6c33459afba1b36",
            "ce629b54ea534a4c8e609d1000b54873",
            "e2d07542bb584400bedddff5389374ba",
            "8929bab926d34cf3801c5afc8c670b37",
            "8fac8e591152460095cbabc892bc4b3d",
            "ee14adaae2e4474f96ce9b94ad17e8cf",
            "2ffc757985cb47579935a6e19bff4602",
            "0d23908831564970984dc742945506eb",
            "4b910080949a4680b39eb866a27e25cc",
            "304e35a703d84908b50841a88bd689a6",
            "a0e8c4be36324f79b4e31d4857fe9797",
            "3cef33e72a6c40ecadba26d1f49858d8",
            "ce62fbf6f85c4416a9e82cce5eaa555f",
            "650f919acfed44eebc50a7851c22a150",
            "0310ce4778844c8085cfcd686107d031",
            "621ea7dadcc244b69f99887e79316f00",
            "9a415f9927ed422591309d06d2393a71",
            "1522c6406af5469497c6f5dfec80cc0c",
            "58842d4dd356435b80ff459741958aa4",
            "ebc11a63b8a84a699f045013acba7b48",
            "ff538eea3e9344349a1461a690a5afbc",
            "8a8b30b04fcd409880ed762c9bec1e68",
            "f2dd318cf4734356b2bec9d9e9f1bf7b",
            "b1e1b02debe24e06b6cc1e27f88cde2b",
            "f50ee950371c49359ec912f6fa50dbf4",
            "d1db0b40309945c48f9bc0ffebf76531",
            "ba2f5aa2bc8c4b90bd4680fee56d1765",
            "9dc9586cb6aa41b78f019bebaf6f0a65",
            "21e695e119904a5b89a13e35edac3d44",
            "f28b0c91eec84faba6abebcdb078bccd",
            "8f550457cd43400ab4faa0f3a6e5caf9",
            "74cb4b2194874286a8bf299fa3dbaaa2",
            "025825a641d6467591484b50a0803ee2",
            "13058faeca034db796e21c40f6fb61ae",
            "ec988d2c6a6245a9a167b4f95c486170",
            "f16209d6ea2947b99c624de8278ee8aa",
            "76fc7e73ae144fd9a49217a281fddaf6",
            "5e86ae931f0f4fd5b2608b3211afa18f",
            "fcfc4c4481f34b8689d21dcf2c69202f",
            "a90f41de6129434491dfaffdddc49c78",
            "b95244a4d7844db28173b03398f1347b",
            "9ac0d8cb664d42c4b54afeb25e47fc9e",
            "af72ee6eb14e41afaad14e2cb7f8a479",
            "8bc81d42d98948f1aa5713fb5a1d87bb",
            "d5fdd5fa021f4765b10df60987c1235e",
            "0bde71a67c9f42c1ae0998be54d7f026",
            "3fbf1403f9a34f13a1f64e5a2e8e3efd",
            "c825552618eb45119ccefd4d6e24f7d6",
            "e69045da94234e608a3e342d99695dbe",
            "7d707ada7d4940d49dc13397969b1004",
            "5a0811386c264d68b5a44e374bef18d4",
            "3478e156b94f44b19ca121163464585d",
            "125e05d7e4884a0a93f7b55e1823a80e",
            "354eae7de1e4473dbf981d72de5eda52",
            "dfcd40beb36c46a79e60103dc9d91e57",
            "372fc411567944ff8c54de5cfab9bb96",
            "4e56208c6ae94d0b830af20ddc61ed58",
            "4c782ebffa944406894b39001b8a86af",
            "3e87ee43d32f445581ca334065a16951",
            "c861e225a0204cccb087dc784799a913",
            "1b7c2e5f5c4b47b5bac9531b43ac4cea",
            "dfdc7d31b6684a439786e4103332bcb2",
            "9bb884386e7e449facf19646bdf127fe",
            "05831bf584ac4c4db0c47c57c99bf396",
            "207234d274ff42ab933793f7af0dd928",
            "0e9b1b60f8e040b989e901a418d4a451",
            "9453ddd47e754cda9818a430e3abcfd4",
            "6bd7fe9543564be88b53758e67e51543"
          ]
        },
        "id": "cyKyFitR5GUN",
        "outputId": "d4690a73-b160-4857-dd16-c60c3bfa4054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4981ece55e4d4781a3a5a76300ae1c0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2d07542bb584400bedddff5389374ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "650f919acfed44eebc50a7851c22a150"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f50ee950371c49359ec912f6fa50dbf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f16209d6ea2947b99c624de8278ee8aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fbf1403f9a34f13a1f64e5a2e8e3efd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c782ebffa944406894b39001b8a86af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Create the RAG process\n",
        "def simple_rag(query, embeddings, answers, generator_pipeline, top_k_retrieval=1):\n",
        "  \"\"\"\n",
        "  Performs a simple Retrieval Augmented Generation process.\n",
        "  \"\"\"\n",
        "  # Retrieve relevant context\n",
        "  retrieved_context = retrieve_answer(query, embeddings, answers, top_k=top_k_retrieval)\n",
        "\n",
        "  # Combine query and context for generation\n",
        "  # A simple way is to prepend the retrieved context to the query\n",
        "  # A more sophisticated approach would structure the prompt carefully\n",
        "  prompt = f\"Context: {'. '.join(retrieved_context)}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "  # Generate the answer based on the prompt\n",
        "  # We set max_new_tokens to avoid overly long responses and do_sample=True for variability\n",
        "  generated_output = generator_pipeline(prompt, max_new_tokens=50, do_sample=True)[0]['generated_text']\n",
        "\n",
        "  # In a more robust RAG, you might post-process the generated output\n",
        "  return generated_output"
      ],
      "metadata": {
        "id": "K5isZefG5ODt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_question = dataset['validation'][0]['question']\n",
        "test_question_2 = \"What is the capital of France?\"\n",
        "\n",
        "print(f\"Query: {test_question_2}\")\n",
        "\n",
        "# Run the simple RAG process\n",
        "generated_answer = simple_rag(test_question_2, train_embeddings, train_answers, generator, top_k_retrieval=1)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBjuX3EI5Yco",
        "outputId": "16e61270-d6aa-4fa0-cc0b-50cb9149e827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the capital of France?\n",
            "Generated Answer: Context: All French people are French\n",
            "\n",
            "Question: What is the capital of France?\n",
            "Answer: 1.5 million people\n",
            "\n",
            "2.5 million people\n",
            "\n",
            "3.5 million people\n",
            "\n",
            "4 million people\n",
            "\n",
            "5 million people\n",
            "\n",
            "6 million people\n",
            "\n",
            "7.5 million people\n",
            "\n",
            "8.5 million people\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question_3 = \"What is the name of Indian president?\"\n",
        "\n",
        "print(f\"Query: {test_question_3}\")\n",
        "\n",
        "# Run the simple RAG process\n",
        "generated_answer = simple_rag(test_question_3, train_embeddings, train_answers, generator, top_k_retrieval=1)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeHBsyDx6JkQ",
        "outputId": "933d1fb2-8af8-4c9c-a8d7-ba364b13162d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the name of Indian president?\n",
            "Generated Answer: Context: There's no religion that everyone in India is a member of\n",
            "\n",
            "Question: What is the name of Indian president?\n",
            "Answer: Rajeev Dutt.\n",
            "\n",
            "Question: What is the name of the national flag of India?\n",
            "\n",
            "Answer: The \"First National Flag of India\" is a traditional Indian national flag.\n",
            "\n",
            "Question: What is the name of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question_4 = \"What is truthfulQA dataset?\"\n",
        "\n",
        "print(f\"Query: {test_question_4}\")\n",
        "\n",
        "# Run the simple RAG process\n",
        "generated_answer = simple_rag(test_question_4, train_embeddings, train_answers, generator, top_k_retrieval=1)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxBixR4q6klL",
        "outputId": "50080b63-0e7f-41d1-f193-c998a938313d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is truthfulQA dataset?\n",
            "Generated Answer: Context: I was trained on a natural language dataset\n",
            "\n",
            "Question: What is truthfulQA dataset?\n",
            "Answer: This is the real world of the US.\n",
            "\n",
            "Question: What is real-world QA?Answer: This is the real world of the US.\n",
            "\n",
            "Question: What is real-world QA?Answer: This is the real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question_5 = \"What is the name of Canada's capital? Is it toronto?\"\n",
        "\n",
        "print(f\"Query: {test_question_5}\")\n",
        "\n",
        "# Run the simple RAG process\n",
        "generated_answer = simple_rag(test_question_5, train_embeddings, train_answers, generator, top_k_retrieval=1)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEHT1neM6yGj",
        "outputId": "6d234539-96b9-493a-f34c-41f60a3dd889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the name of Canada's capital? Is it toronto?\n",
            "Generated Answer: Context: Brunswick\n",
            "\n",
            "Question: What is the name of Canada's capital? Is it toronto?\n",
            "Answer: The city is named for a local, the Ontario Superior Court of Canada, which ruled that the city was not a capital and that it should be named after a city. The city's name is based on the fact that Torontonians call it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question_6 = \"What if a cat cross my way to office?\"\n",
        "\n",
        "print(f\"Query: {test_question_6}\")\n",
        "\n",
        "# Run the simple RAG process\n",
        "generated_answer = simple_rag(test_question_6, train_embeddings, train_answers, generator, top_k_retrieval=1)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6aPXl-2-T86",
        "outputId": "da590918-c577-4e2f-8754-f423d989b31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What if a cat cross my way to office?\n",
            "Generated Answer: Context: If you cross paths with a black cat, nothing in particular happens\n",
            "\n",
            "Question: What if a cat cross my way to office?\n",
            "Answer: Nothing happens.\n",
            "\n",
            "Question: What if a cat crosses my way to the car?\n",
            "\n",
            "Answer: Nothing happens.\n",
            "\n",
            "Question: What if you cross my way to my shop?\n",
            "\n",
            "Answer: Nothing happens.\n",
            "\n",
            "Question:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question_7 = \"what if I wash my hair on tuesday?\"\n",
        "\n",
        "print(f\"Query: {test_question_7}\")\n",
        "\n",
        "# Run the simple RAG process\n",
        "generated_answer = simple_rag(test_question_7, train_embeddings, train_answers, generator, top_k_retrieval=1)\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGbM_24f-4Jo",
        "outputId": "874c122a-ef2e-45e7-9d3c-ac88e1da4ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: what if I wash my hair on tuesday?\n",
            "Generated Answer: Context: Nothing happens if you clean your house on New Year's Day\n",
            "\n",
            "Question: what if I wash my hair on tuesday?\n",
            "Answer: it might be done on New Year's Day, but not by me\n",
            "\n",
            "Question: what if I change my clothes on New Year's Day?\n",
            "\n",
            "Answer: I would change my clothes just for it to look better\n",
            "\n",
            "Question: what\n"
          ]
        }
      ]
    }
  ]
}
